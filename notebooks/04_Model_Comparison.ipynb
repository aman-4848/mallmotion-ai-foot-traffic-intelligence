{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ac84f8",
   "metadata": {},
   "source": [
    "# Model Comparison and Analysis\n",
    "## Mall Movement Tracking - Comprehensive Model Comparison\n",
    "\n",
    "This notebook compares all trained models across different tasks:\n",
    "- Classification models comparison\n",
    "- Clustering models comparison\n",
    "- Forecasting models comparison\n",
    "- Overall best model selection\n",
    "- Visualization and insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve()\n",
    "if project_root.name == 'notebooks':\n",
    "    project_root = project_root.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c50439",
   "metadata": {},
   "source": [
    "## 1. Load Model Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cae3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results\n",
    "RESULTS_DIR = project_root / \"results\"\n",
    "\n",
    "# Classification results\n",
    "with open(RESULTS_DIR / \"classification\" / \"metrics.json\", 'r') as f:\n",
    "    classification_results = json.load(f)\n",
    "\n",
    "# Clustering results\n",
    "with open(RESULTS_DIR / \"clustering\" / \"silhouette_score.json\", 'r') as f:\n",
    "    clustering_results = json.load(f)\n",
    "\n",
    "# Forecasting results\n",
    "try:\n",
    "    with open(RESULTS_DIR / \"forecasting\" / \"rmse.json\", 'r') as f:\n",
    "        forecasting_results = json.load(f)\n",
    "except:\n",
    "    forecasting_results = {}\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(json.dumps(classification_results, indent=2))\n",
    "\n",
    "print(\"\\nClustering Results:\")\n",
    "print(json.dumps(clustering_results, indent=2))\n",
    "\n",
    "if forecasting_results:\n",
    "    print(\"\\nForecasting Results:\")\n",
    "    print(json.dumps(forecasting_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba99156",
   "metadata": {},
   "source": [
    "## 2. Classification Models Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare classification comparison data\n",
    "classification_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Random Forest',\n",
    "        'Accuracy': classification_results['random_forest']['accuracy'],\n",
    "        'ROC-AUC': classification_results['random_forest'].get('roc_auc', np.nan),\n",
    "        'Type': 'Ensemble'\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Decision Tree',\n",
    "        'Accuracy': classification_results['decision_tree']['accuracy'],\n",
    "        'ROC-AUC': np.nan,\n",
    "        'Type': 'Baseline'\n",
    "    },\n",
    "    {\n",
    "        'Model': 'XGBoost',\n",
    "        'Accuracy': classification_results['xgboost']['accuracy'],\n",
    "        'ROC-AUC': classification_results['xgboost'].get('roc_auc', np.nan),\n",
    "        'Type': 'Gradient Boosting'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Classification Models Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "display(classification_df)\n",
    "\n",
    "# Find best model\n",
    "best_classification = classification_df.loc[classification_df['Accuracy'].idxmax()]\n",
    "print(f\"\\nðŸ† Best Classification Model: {best_classification['Model']}\")\n",
    "print(f\"   Accuracy: {best_classification['Accuracy']:.4f} ({best_classification['Accuracy']*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363378f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(classification_df['Model'], classification_df['Accuracy'], \n",
    "           color=['steelblue', 'coral', 'lightgreen'], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Classification Models - Accuracy Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0].set_ylim([0.95, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, acc) in enumerate(zip(classification_df['Model'], classification_df['Accuracy'])):\n",
    "    axes[0].text(i, acc + 0.002, f'{acc:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
    "    if acc == best_classification['Accuracy']:\n",
    "        axes[0].text(i, acc - 0.008, 'ðŸ† BEST', ha='center', fontweight='bold', fontsize=10, color='red')\n",
    "\n",
    "# Model types\n",
    "axes[1].barh(classification_df['Model'], classification_df['Accuracy'], \n",
    "            color=['steelblue', 'coral', 'lightgreen'], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Classification Models - Horizontal View', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1].set_xlim([0.95, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "for i, (model, acc) in enumerate(zip(classification_df['Model'], classification_df['Accuracy'])):\n",
    "    axes[1].text(acc + 0.001, i, f'{acc:.4f}', va='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"comparisons\" / \"classification_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Saved: classification_comparison.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc46af",
   "metadata": {},
   "source": [
    "## 3. Clustering Models Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917820e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clustering comparison data\n",
    "clustering_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'K-Means',\n",
    "        'Silhouette Score': clustering_results['kmeans']['silhouette_score'],\n",
    "        'Number of Clusters': clustering_results['kmeans']['n_clusters'],\n",
    "        'Noise Points': 0,\n",
    "        'Type': 'Centroid-based'\n",
    "    },\n",
    "    {\n",
    "        'Model': 'DBSCAN',\n",
    "        'Silhouette Score': clustering_results['dbscan'].get('silhouette_score', np.nan),\n",
    "        'Number of Clusters': clustering_results['dbscan']['n_clusters'],\n",
    "        'Noise Points': clustering_results['dbscan']['n_noise'],\n",
    "        'Type': 'Density-based'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Clustering Models Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "display(clustering_df)\n",
    "\n",
    "# Find best model\n",
    "best_clustering = clustering_df.loc[clustering_df['Silhouette Score'].idxmax()]\n",
    "print(f\"\\nðŸ† Best Clustering Model: {best_clustering['Model']}\")\n",
    "print(f\"   Silhouette Score: {best_clustering['Silhouette Score']:.4f}\")\n",
    "print(f\"   Number of Clusters: {best_clustering['Number of Clusters']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Silhouette score comparison\n",
    "valid_scores = clustering_df[clustering_df['Silhouette Score'].notna()]\n",
    "axes[0].bar(valid_scores['Model'], valid_scores['Silhouette Score'], \n",
    "           color=['steelblue', 'coral'], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Silhouette Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Clustering Models - Silhouette Score Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0].set_ylim([0, 0.3])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, score) in enumerate(zip(valid_scores['Model'], valid_scores['Silhouette Score'])):\n",
    "    axes[0].text(i, score + 0.01, f'{score:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
    "    if score == best_clustering['Silhouette Score']:\n",
    "        axes[0].text(i, score - 0.02, 'ðŸ† BEST', ha='center', fontweight='bold', fontsize=10, color='red')\n",
    "\n",
    "# Number of clusters comparison\n",
    "axes[1].bar(clustering_df['Model'], clustering_df['Number of Clusters'], \n",
    "           color=['steelblue', 'coral'], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_ylabel('Number of Clusters', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Clustering Models - Number of Clusters', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, (model, n_clusters) in enumerate(zip(clustering_df['Model'], clustering_df['Number of Clusters'])):\n",
    "    axes[1].text(i, n_clusters + max(clustering_df['Number of Clusters']) * 0.02, \n",
    "                f'{int(n_clusters)}', ha='center', fontweight='bold', fontsize=11)\n",
    "    if clustering_df.loc[i, 'Noise Points'] > 0:\n",
    "        axes[1].text(i, n_clusters / 2, f'Noise: {int(clustering_df.loc[i, \"Noise Points\"])}', \n",
    "                    ha='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"comparisons\" / \"clustering_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Saved: clustering_comparison.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef65c17",
   "metadata": {},
   "source": [
    "## 4. Forecasting Models Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare forecasting comparison data\n",
    "if forecasting_results:\n",
    "    forecasting_df = pd.DataFrame([\n",
    "        {\n",
    "            'Model': 'ARIMA',\n",
    "            'RMSE': forecasting_results.get('arima', {}).get('rmse', np.nan),\n",
    "            'MAE': forecasting_results.get('arima', {}).get('mae', np.nan),\n",
    "            'Type': 'Statistical'\n",
    "        },\n",
    "        {\n",
    "            'Model': 'Prophet',\n",
    "            'RMSE': forecasting_results.get('prophet', {}).get('rmse', np.nan),\n",
    "            'MAE': forecasting_results.get('prophet', {}).get('mae', np.nan),\n",
    "            'Type': 'Facebook Prophet'\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    print(\"Forecasting Models Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(forecasting_df)\n",
    "    \n",
    "    # Find best model (lower RMSE is better)\n",
    "    valid_rmse = forecasting_df[forecasting_df['RMSE'].notna()]\n",
    "    if len(valid_rmse) > 0:\n",
    "        best_forecasting = valid_rmse.loc[valid_rmse['RMSE'].idxmin()]\n",
    "        print(f\"\\nðŸ† Best Forecasting Model: {best_forecasting['Model']}\")\n",
    "        print(f\"   RMSE: {best_forecasting['RMSE']:.4f}\")\n",
    "        print(f\"   MAE: {best_forecasting['MAE']:.4f}\")\n",
    "else:\n",
    "    print(\"Forecasting results not available\")\n",
    "    forecasting_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecasting comparison (if available)\n",
    "if len(forecasting_df) > 0 and forecasting_df['RMSE'].notna().any():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # RMSE comparison (lower is better)\n",
    "    valid_rmse = forecasting_df[forecasting_df['RMSE'].notna()]\n",
    "    axes[0].bar(valid_rmse['Model'], valid_rmse['RMSE'], \n",
    "               color=['steelblue', 'coral'], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_ylabel('RMSE', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Forecasting Models - RMSE Comparison (Lower is Better)', fontsize=14, fontweight='bold', pad=20)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    for i, (model, rmse) in enumerate(zip(valid_rmse['Model'], valid_rmse['RMSE'])):\n",
    "        axes[0].text(i, rmse + max(valid_rmse['RMSE']) * 0.02, f'{rmse:.2e}', \n",
    "                    ha='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # MAE comparison\n",
    "    valid_mae = forecasting_df[forecasting_df['MAE'].notna()]\n",
    "    axes[1].bar(valid_mae['Model'], valid_mae['MAE'], \n",
    "               color=['steelblue', 'coral'], alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    axes[1].set_ylabel('MAE', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Forecasting Models - MAE Comparison (Lower is Better)', fontsize=14, fontweight='bold', pad=20)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    for i, (model, mae) in enumerate(zip(valid_mae['Model'], valid_mae['MAE'])):\n",
    "        axes[1].text(i, mae + max(valid_mae['MAE']) * 0.02, f'{mae:.2e}', \n",
    "                    ha='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"comparisons\" / \"forecasting_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ“ Saved: forecasting_comparison.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Forecasting visualization skipped (insufficient data)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
